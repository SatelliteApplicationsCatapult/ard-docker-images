{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI\n",
    "import importlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC Hackathon ARD Preparation Master Template\n",
    "\n",
    "In this notebook all the major functions for processing (COG + sen2cor) and preparing (yaml creation) for S1 and S2 ARD for ingestion + use within a DC (CIAB).\n",
    "\n",
    "Structure of the notebook is as follows:\n",
    "- Objectives of preparation activities\n",
    "- Arranging the S2 processing blocks\n",
    "- Template S2 processing & preparation\n",
    "- Template S1 processing & preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of preparation activities (in priority order):\n",
    "- process & upload all L2A for granules x,y,z,a in 2019 (approx 200 scenes)\n",
    "- process & upload all L1C for granules x,y,z,a in last six months of 2018 (approx 240 scenes) to give for annual signature\n",
    "\n",
    "- [ process & upload all S1 for first six months of 2018 (to give full 2018 signature) - ***nice to have*** ]\n",
    "- [ process & upload all L2C for first six months of 2018 (approx another 240 scenes) to give full 2018 signature - ***nice to have*** ]\n",
    "\n",
    "remember to amend permissions of bucket directories\n",
    "remember this exists... https://registry.opendata.aws/sentinel-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOI for all 4 S2 granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wkt_aoi = \"POLYGON((176 -15,-178 -15,-178 -20,176 -20,176 -15))\" # main fiji test\n",
    "#wkt_aoi = \"POLYGON((178.45 -17.67,178.58 -17.67,178.58 -17.78,178.45 -17.78,178.45 -17.67))\" # first granule east coast\n",
    "wkt_aoi = \"POLYGON((177.52 -17.54,178.37 -17.54,178.37 -18.31,177.52 -18.31,177.52 -17.54))\" # four granules for hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2 L2A products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assumed that l1c must be processed for all products before 2019. not necessarily true, l2a is back to just before 2019 but is a nice window to aim for...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query ESA Open Access Hub using sentinelsat tool.\n",
    "esa_api = SentinelAPI('tmj21','Welcome12!')\n",
    "#res = esa_api.query(wkt_aoi,platformname='Sentinel-2',producttype='S2MSI1C')\n",
    "#res = esa_api.to_geodataframe(res)\n",
    "res = esa_api.query(wkt_aoi,\n",
    "                    platformname='Sentinel-2',\n",
    "                    producttype='S2MSI2A',\n",
    "                    date = (\"20190101\", \"20190601\")\n",
    "                    #date = (20190507), date(2015, 12, 29)\n",
    "                   )\n",
    "res = esa_api.to_geodataframe(res)\n",
    "print (res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop already processing granule...\n",
    "res['tileid'] = res.title.str[-22:-16]\n",
    "res.sort_values(by ='beginposition')\n",
    "print ( np.unique(res.tileid.values) )\n",
    "print ( len(res.title.values) )\n",
    "print ( res.title.values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hack_scripts.hackprep_helper_functions import *\n",
    "\n",
    "non_cog_dir = \"../S2_COG_TEST/non_cog/\"\n",
    "cog_dir = \"../S2_COG_TEST/cog/\"\n",
    "s3_bucket = \"test.data.frontiersi.io\"\n",
    "s3_dir = \"catapult/sentinel2/rolling_w_esa/\"\n",
    "\n",
    "n = 0\n",
    "\n",
    "# eventual MASTER S2 L2A function requiring esa query df and working directoy\n",
    "for d_id, original_scene_dir, scene_name in zip(res.uuid.values, res.filename.values, res.title):\n",
    "    # print ( d_id, original_scene_dir, scene_name )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "    #print (scene_dir)\n",
    "    \n",
    "    tileid = original_scene_dir.split('_')[-2]\n",
    "    \n",
    "    # shorten scene name - wayyyy too long...\n",
    "    scene_name = scene_name[:26] + '_' + tileid\n",
    "    print ( '#### SCENE {} {} of {} ####'.format(scene_name, n, res.shape[0]) )\n",
    "    \n",
    "    # create full path for original and cog scenes\n",
    "    original_scene_dir = non_cog_dir + original_scene_dir + '/'\n",
    "    scene_dir = cog_dir + scene_name + '/'\n",
    "    print ( 'Target original scene dir: {}'.format(original_scene_dir) )\n",
    "    print ( 'Tarket scene dir: {}'.format(scene_dir) )\n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "    \n",
    "    download_extract_s2(d_id, non_cog_dir, original_scene_dir)\n",
    " \n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "\n",
    "    # create (or clear) cog scene directory\n",
    "    if not os.path.exists(scene_dir):\n",
    "        print ( 'Creating scene cog directory: {}'.format(scene_dir) )\n",
    "        os.mkdir(scene_dir)\n",
    "    else:\n",
    "        print ( 'Scene cog directory already exists so passing: {}'.format(scene_dir) )\n",
    "    \n",
    "    copy_s2_metadata(original_scene_dir, scene_dir, scene_name)\n",
    "\n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "    \n",
    "    conv_s2scene_cogs(original_scene_dir, scene_dir, scene_name)\n",
    "\n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "\n",
    "    create_yaml(scene_dir, 's2')\n",
    "    \n",
    "    # s3_upload_cogs(glob.glob(scene_dir + '*'), s3_bucket, s3_dir)\n",
    "    \n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "    \n",
    "    print ( 'Nuking original directory: {}'.format(original_scene_dir) )\n",
    "    # shutil.rmtree(original_scene_dir)\n",
    "    \n",
    "    print ( 'Nuking intermediary scene cog dir: {}'.format(scene_dir) )\n",
    "    \n",
    "    print ( ' #### COMPLETED #### '.format(scene_name) )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2 L1C products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assumed that l1c must be processed for all products before 2019. not necessarily true, l2a is back to just before 2019 but is a nice window to aim for...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query ESA Open Access Hub using sentinelsat tool.\n",
    "esa_api = SentinelAPI('tmj21','Welcome12!')\n",
    "#res = esa_api.query(wkt_aoi,platformname='Sentinel-2',producttype='S2MSI1C')\n",
    "#res = esa_api.to_geodataframe(res)\n",
    "res = esa_api.query(wkt_aoi,\n",
    "                    platformname='Sentinel-2',\n",
    "                    producttype='S2MSI1C',\n",
    "                    date = (\"20180701\", \"20190101\")\n",
    "                    #date = (20190507), date(2015, 12, 29)\n",
    "                   )\n",
    "res = esa_api.to_geodataframe(res)\n",
    "print (res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop already processing granule...\n",
    "res['tileid'] = res.title.str[-22:-16]\n",
    "res = res.sort_values(by ='beginposition', ascending=False)\n",
    "print ( np.unique(res.tileid.values) )\n",
    "print ( len(res.title.values) )\n",
    "print ( res.title.values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hack_scripts.hackprep_helper_functions import *\n",
    "\n",
    "non_cog_dir = \"../S2_sen2cor_COG/non_cog/\"\n",
    "cog_dir = \"../S2_sen2cor_COG/cog/\"\n",
    "s3_bucket = \"test.data.frontiersi.io\"\n",
    "s3_dir = \"catapult/sentinel2/rolling_w_esa/\"\n",
    "sen2cor = \"/data/CommonSensing/Code/Sen2Cor-02.08.00-Linux64/bin/L2A_Process\"\n",
    "\n",
    "n = 0\n",
    "\n",
    "# eventual MASTER S2 L2A function requiring esa query df and working directoy\n",
    "for d_id, original_scene_dir, scene_name in zip(res.uuid.values, res.filename.values, res.title):\n",
    "    # print ( d_id, original_scene_dir, scene_name )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "    #print (scene_dir)\n",
    "    \n",
    "    tileid = original_scene_dir.split('_')[-2]\n",
    "    \n",
    "    # shorten scene name - wayyyy too long...\n",
    "    scene_name = scene_name[:26] + '_' + tileid\n",
    "    print ( '#### SCENE {} {} of {} ####'.format(scene_name, n, res.shape[0]) )\n",
    "    \n",
    "    # create full path for original and cog scenes\n",
    "    original_scene_dir = non_cog_dir + original_scene_dir + '/'\n",
    "    scene_dir = cog_dir + scene_name + '/'\n",
    "    print ( 'Target original scene dir: {}'.format(original_scene_dir) )\n",
    "    print ( 'Tarket scene dir: {}'.format(scene_dir) )\n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "    \n",
    "    download_extract_s2(d_id, non_cog_dir, original_scene_dir)\n",
    " \n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "\n",
    "    l2a_dir = original_scene_dir.replace('_MSIL1C', '_MSIL2A')[:-39]\n",
    "    \n",
    "    # see if l2a prod exists to avoid running sen2cor unless needed\n",
    "    l2a_dir = glob.glob(original_scene_dir.replace('_MSIL1C', '_MSIL2A')[:-39]+'*')[0]+'/'\n",
    "    print ('pre', l2a_dir )\n",
    "    \n",
    "    if not os.path.exists(l2a_dir):\n",
    "    \n",
    "        sen2cor_correction(sen2cor, original_scene_dir, non_cog_dir)\n",
    "        # shutil.rmtree(original_scene_dir)\n",
    "\n",
    "        l2a_dir = glob.glob(original_scene_dir.replace('_MSIL1C', '_MSIL2A')[:-39]+'*')[0] +'/'\n",
    "        print ('re-defining original to l2a: {}'.format(l2a_dir) )\n",
    "    \n",
    "    original_scene_dir = l2a_dir\n",
    "    \n",
    "    # create (or clear) cog scene directory\n",
    "    if not os.path.exists(scene_dir):\n",
    "        print ( 'Creating scene cog directory: {}'.format(scene_dir) )\n",
    "        os.mkdir(scene_dir)\n",
    "    else:\n",
    "        print ( 'Scene cog directory already exists so passing: {}'.format(scene_dir) )\n",
    "    \n",
    "    copy_s2_metadata(original_scene_dir, scene_dir, scene_name)\n",
    "\n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "    \n",
    "    conv_s2scene_cogs(original_scene_dir, scene_dir, scene_name)\n",
    "\n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "\n",
    "    create_yaml(scene_dir, 's2')\n",
    "    \n",
    "    s3_upload_cogs(glob.glob(scene_dir + '*'), s3_bucket, s3_dir)\n",
    "    \n",
    "    print ( 'Time: {}'.format(str(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))) )\n",
    "    \n",
    "    print ( 'Nuking original directory: {}'.format(original_scene_dir) )\n",
    "    # shutil.rmtree(original_scene_dir)\n",
    "    \n",
    "    print ( 'Nuking intermediary scene cog dir: {}'.format(scene_dir) )\n",
    "    \n",
    "    print ( ' #### COMPLETED #### '.format(scene_name) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
